
This note is intended for branch params_dlrm. The purpose of this branch is to test DLRM with large data models.

The data is configured with total 1359 embedding tables, including user related 815 tables with hash size 26000000 and
ads related tables with hash size 14000000. You could set up the parameters in the shell script as following:

large_arch_emb_usr=$(printf '26000000%.0s' {1..815})
large_arch_emb_usr=${large_arch_emb_usr//"03"/"0-3"}

large_arch_emb_ads=$(printf '14000000%.0s' {1..544})
large_arch_emb_ads=${large_arch_emb_ads//"04"/"0-4"}

large_arch_emb="$large_arch_emb_usr-$large_arch_emb_ads"

There are a new parameter which do not exist in master branch:
--arch-project-size : reduces the number of interaction features for the dot operation. 
   This is mainly due to the memory concern. It reduces the memory size needed for top MLP. 

Also, the data-generation supports one more data model "fb_synthetic".
   
The model parameter size depends on the mini-batch-size and data types.
Suppose, we set mini-batch-size=2048, data type=float32,
The model size for embedding tables are about 14TB.
The MLP model size are about 8.5GB per copy. 

Here are the command line to run the large model dlrm:
python dlrm_s_pytorch.py 
   --arch-sparse-feature-size=128 
   --arch-mlp-bot="2000-1024-1024-1024-1024-1024-1024-1024-1024-1024-1024-512-128" 
   --arch-mlp-top="4096-4096-4096-4096-4096-4096-4096-4096-4096-4096-4096-4096-4096-1" 
   --arch-embedding-size=$large_arch_emb 
   --data-generation=rando 
   --loss-function=bce 
   --round-targets=True 
   --learning-rate=0.1 
   --mini-batch-size=2048 
   --print-freq=10240 
   --print-time 
   --test-mini-batch-size=16384 
   --test-num-workers=16
   --num-indices-per-lookup-fixed=1 
   --num-indices-per-lookup=28
   --arch-projection-size 30
   --use-gpu


